#!/bin/sh

# Dependencies - webtorrent, mpv

mkdir -p $HOME/.cache/notflix

DOWNLOAD_DIR="$HOME/Documents/Torrent"

baseurl="https://1337x.wtf"

cachedir="$HOME/.cache/notflix"

LOG_FILE="$HOME/.cache/notflix/notflix_history"

[[ -f "$LOG_FILE" ]] && LS="$(cat $LOG_FILE)" 
[[ -z "$LS" ]] && LS=""

PAGE=1

scrape()
{

S_QRY="$(echo "$QUER_Y" | sed 's/[[:space:]]/_/g')"

menu="fzf --no-preview --cycle --layout=reverse --header-first --header=Torrent-Results:($S_QRY/Page-$PAGE)"

[[ -z "$QUER_Y" ]] && exit

query="$(echo "$QUER_Y" | sed 's/ /+/g')"

#curl -s https://1337x.to/category-search/$query/Movies/1/ > $cachedir/tmp.html
curl -s $baseurl/search/$query/$PAGE/ --compressed > $cachedir/tmp.html

# Get Titles
grep -o '<a href="/torrent/.*</a>' $cachedir/tmp.html |
  sed 's/<[^>]*>//g' > $cachedir/titles.bw

result_count=$(wc -l $cachedir/titles.bw | awk '{print $1}')
if [ "$result_count" -lt 1 ]; then
 echo "No Result found!" 
 exit 0
fi

# Seeders and Leechers
grep -o '<td class="coll-2 seeds.*</td>\|<td class="coll-3 leeches.*</td>' $cachedir/tmp.html |
  sed 's/<[^>]*>//g' | sed 'N;s/\n/ /' > $cachedir/seedleech.bw

# Size
grep -o '<td class="coll-4 size.*</td>' $cachedir/tmp.html |
  sed 's/<span class="seeds">.*<\/span>//g' |
  sed -e 's/<[^>]*>//g' > $cachedir/size.bw

# Links
grep -E '/torrent/' $cachedir/tmp.html |
  sed -E 's#.*(/torrent/.*)/">.*/#\1#' |
  sed 's/td>//g' > $cachedir/links.bw

# Clearning up some data to display
sed 's/\./ /g; s/\-/ /g' $cachedir/titles.bw |
  sed 's/[^A-Za-z0-9 ]//g' | tr -s " " > $cachedir/tmp && mv $cachedir/tmp $cachedir/titles.bw

awk '{print NR " - ["$0"]"}' $cachedir/size.bw > $cachedir/tmp && mv $cachedir/tmp $cachedir/size.bw
awk '{print "[S:"$1 ", L:"$2"]" }' $cachedir/seedleech.bw > $cachedir/tmp && mv $cachedir/tmp $cachedir/seedleech.bw

[[ "$PAGE" > 1 ]] && echo "Previous Page" >> $cachedir/titles.bw 

echo "Next Page" >> $cachedir/titles.bw

# Getting the line number
LINEO=$(paste -d\   $cachedir/size.bw $cachedir/seedleech.bw $cachedir/titles.bw | sed 's/^ //g' | $menu )

LINE=$( echo "$LINEO" | cut -d\- -f1 | awk '{$1=$1; print}')

if [ -z "$LINE" ]; then
exit 0 
fi 

# Next Page
[[ "$LINE" = "Next Page" ]] && PAGE=$(($PAGE+1)) && scrape

#Previous Page
[[ "$LINE" = "Previous Page" ]] && PAGE="$(($PAGE-1))" && scrape


url=$(head -n $LINE $cachedir/links.bw | tail -n +$LINE)
fullURL="${baseurl}${url}/"

# Requesting page for magnet link
curl -s $fullURL > $cachedir/tmp.html
magnet=$(grep -Po "magnet:\?xt=urn:btih:[a-zA-Z0-9]*" $cachedir/tmp.html | head -n 1) 

[[ -z "$magnet" ]] && echo "Can't Get the Link!" && exit

PROMPTO="$(echo -e "Open\nDownload" | $menu)"

LOG()
{
  echo "$LINEO" > $LOG_FILE
}

case $PROMPTO in
  Open)
    LOG
    webtorrent "$magnet" --mpv 
    exit
    ;;
  Download) 
    echo -e "Downloading $S_QRY in $DOWNLOAD_DIR\n"
    LOG
    webtorrent download --verbose "$magnet" --out "$DOWNLOAD_DIR"
    exit
    ;;
  *)
    ;;

  esac

exit

}



[[ -z "$@" ]] && read -r -p "Last Torrent: $LS
Search Torrent: " QUER_Y && scrape || QUER_Y="$@" && scrape 

